{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "29571d2f5f6d47098e0c69b07616734a",
    "deepnote_cell_type": "code",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 0,
    "execution_start": 1746785309355,
    "source_hash": "39219d5e"
   },
   "outputs": [],
   "source": [
    "# Models to try based on the results\n",
    "# 1. Random Forest Classifier\n",
    "# 2. LightGBM Classifier\n",
    "# 3. XGBoost Classifier\n",
    "# 4. LinearDiscriminantAnalysis\n",
    "# 5. KNN?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_validate, LeaveOneOut\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "cfdef7374b6a48998003d74987ebf89d",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 213888,
    "execution_start": 1746785311374,
    "source_hash": "808af377",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_evaluate_classifiers(X_train, y_train, X_test, y_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers using 5-fold cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing results for each classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define classifiers\n",
    "    classifiers = {\n",
    "        \"Random Forest\": {\n",
    "            \"model\": RandomForestClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'max_depth': np.arange(3, 11, dtype=int),\n",
    "                'min_samples_split': np.linspace(0.1, 1.0, 10),\n",
    "                'min_samples_leaf': np.linspace(0.1, 0.5, 5),\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "                'bootstrap': [True, False],\n",
    "            }\n",
    "        },\n",
    "        \"LDA\": {\n",
    "            \"model\": LinearDiscriminantAnalysis(),\n",
    "            \"params\": {\n",
    "                'solver': ['svd', 'lsqr', 'eigen'],\n",
    "                'shrinkage': [None, 'auto'],\n",
    "                'priors': [None, [0.5, 0.5]],\n",
    "                'tol': np.linspace(1e-4, 1e-2, 10),\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBClassifier(\n",
    "                random_state=random_state, \n",
    "                eval_metric='mlogloss',\n",
    "\n",
    "            ),\n",
    "            \"params\": {\n",
    "                'max_depth': np.linspace(3, 11, dtype=int),\n",
    "                'learning_rate': np.linspace(0.1, 0.3, 10),\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'gamma': np.linspace(0, 5, 6),\n",
    "            }\n",
    "        },\n",
    "        \"KNN\": {\n",
    "            \"model\": KNeighborsClassifier(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': np.arange(1, 31),\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'leaf_size': np.arange(10, 100, 10),\n",
    "                \"p\": [1, 2]\n",
    "            }\n",
    "        },\n",
    "        \"LGBM\": {\n",
    "            \"model\": LGBMClassifier(random_state=random_state, verbosity=-1, verbose=-1, device='gpu'),\n",
    "            \"params\": {\n",
    "                'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'max_depth': np.arange(3, 11, dtype=int),\n",
    "                'num_leaves': np.arange(20, 150, 20),\n",
    "                'subsample': np.linspace(0.5, 1.0, 6),\n",
    "                'colsample_bytree': np.linspace(0.5, 1.0, 6),\n",
    "                'reg_alpha': np.linspace(0, 1, 5),\n",
    "                'reg_lambda': np.linspace(0, 1, 5),\n",
    "                'boosting_type': ['gbdt', 'dart'],\n",
    "            }\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Define custom scoring functions with zero_division=0\n",
    "    def precision_scorer(y_true, y_pred):\n",
    "        return precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    def recall_scorer(y_true, y_pred):\n",
    "        return recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    def f1_scorer(y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    # Create scorers\n",
    "    scoring = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision_macro': make_scorer(precision_scorer),\n",
    "        'recall_macro': make_scorer(recall_scorer),\n",
    "        'f1_macro': make_scorer(f1_scorer)\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for name, clf_dict in classifiers.items():\n",
    "        print(f\"Training {name} classifier...\")\n",
    "        # Start timing\n",
    "        start_time = time()\n",
    "        \n",
    "        # Get the model from the dictionary\n",
    "        clf = clf_dict['model']\n",
    "        \n",
    "        # For small datasets or datasets with tiny classes\n",
    "        if X_train.shape[0] < 500:\n",
    "            cv = LeaveOneOut()  # Use LOO for very small datasets\n",
    "        else:\n",
    "            # Count samples in smallest class\n",
    "            class_counts = y_train.value_counts()\n",
    "            min_class_samples = class_counts.min()\n",
    "            \n",
    "\n",
    "            n_splits = min(5, min_class_samples)  # Use at most 5 splits, but no more than samples in smallest class\n",
    "            print(f\"Using {n_splits} splits due to small class size ({min_class_samples} samples in smallest class)\")\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "            # # Choose appropriate number of splits based on smallest class\n",
    "            # if min_class_samples < 10:\n",
    "            #     n_splits = min(5, min_class_samples)  # Use at most 5 splits, but no more than samples in smallest class\n",
    "            #     print(f\"Using {n_splits} splits due to small class size ({min_class_samples} samples in smallest class)\")\n",
    "            #     cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            # else:\n",
    "            #     cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "        \n",
    "        # Perform cross-validation with random search\n",
    "        cv_results = RandomizedSearchCV(clf, \n",
    "                                        clf_dict['params'], \n",
    "                                        cv=cv, \n",
    "                                        scoring=scoring, \n",
    "                                        refit='f1_macro', \n",
    "                                        n_iter=20,\n",
    "                                        n_jobs=-1,\n",
    "                                        random_state=random_state, \n",
    "                                        verbose=1)\n",
    "        cv_results.fit(X_train, y_train)\n",
    "        \n",
    "        # Fit on full training data and evaluate on test set\n",
    "        best_clf = cv_results.best_estimator_\n",
    "        test_pred_encoded = best_clf.predict(X_test)\n",
    "\n",
    "        test_scores = precision_recall_fscore_support(y_test.values, test_pred_encoded, average='macro', zero_division=0)\n",
    "        test_accuracy = accuracy_score(y_test.values, test_pred_encoded)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'cv_accuracy': cv_results.cv_results_['mean_test_accuracy'].mean(),\n",
    "            'cv_accuracy_std': cv_results.cv_results_['std_test_accuracy'].mean(),\n",
    "            'cv_precision': cv_results.cv_results_['mean_test_precision_macro'].mean(),\n",
    "            'cv_recall': cv_results.cv_results_['mean_test_recall_macro'].mean(),\n",
    "            'cv_f1': cv_results.cv_results_['mean_test_f1_macro'].mean(),\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_scores[0],\n",
    "            'test_recall': test_scores[1],\n",
    "            'test_f1': test_scores[2],\n",
    "            'training_time': time() - start_time,\n",
    "            'fitted_model': best_clf,\n",
    "            'best_params': cv_results.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"DONE: {results[name]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "RESULTS = {}\n",
    "\n",
    "# Train and test the model for all datasets\n",
    "approaches = {'Initial': ['real_data', 'synth_data'], 'Extra': ['real_data', 'real_pseudoreal_data', 'real_pseudoreal_synth_data']}\n",
    "\n",
    "for approach, datasets in approaches.items():\n",
    "    for dataset in datasets:\n",
    "        # Load the preprocessed data\n",
    "        X_train = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/X_train.csv\")\n",
    "        y_train = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/y_train.csv\")\n",
    "        X_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/X_test.csv\")\n",
    "        y_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/y_test.csv\")\n",
    "        \n",
    "\n",
    "        print(f\"\\nEvaluating on {approach}_{dataset} dataset:\")\n",
    "\n",
    "        # Train and evaluate\n",
    "        results = train_evaluate_classifiers(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Store results\n",
    "        RESULTS[f\"{approach}_{dataset}\"] = results\n",
    "\n",
    "        # Print results\n",
    "        for clf_name, metrics in results.items():\n",
    "            print(f\"\\n{clf_name}:\")\n",
    "            print(f\"CV Accuracy: {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f})\")\n",
    "            print(f\"Test Accuracy: {metrics['test_accuracy']:.3f}\")\n",
    "            print(f\"Test F1-Score: {metrics['test_f1']:.3f}\")\n",
    "            print(f\"Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "\n",
    "        results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "        print(results_df)\n",
    "    \n",
    "# # Save results to global variable\n",
    "# import pickle\n",
    "# with open(\"results.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "9335436e7ba5443faff31489f2c6a56c",
    "deepnote_cell_type": "visualization",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 1,
    "execution_start": 1746786603404,
    "source_hash": "b623e53d"
   },
   "outputs": [],
   "source": [
    "for dataset_name, dataset_results in RESULTS.items():\n",
    "    print(f\"Results for {dataset_name.capitalize()} Dataset:\\n\")\n",
    "    for clf_name, metrics in dataset_results.items():\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        print(f\"  CV Accuracy: {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f})\")\n",
    "        print(f\"  CV Precision: {metrics['cv_precision']:.3f}\")\n",
    "        print(f\"  CV Recall: {metrics['cv_recall']:.3f}\")\n",
    "        print(f\"  CV F1-Score: {metrics['cv_f1']:.3f}\")\n",
    "        print(f\"  Test Accuracy: {metrics['test_accuracy']:.3f}\")\n",
    "        print(f\"  Test F1-Score: {metrics['test_f1']:.3f}\")\n",
    "        print(f\"  Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "        print(f\"  Best Parameters: {metrics['best_params']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "68543a86dde043478b6cfc9492519366",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 188,
    "execution_start": 1746785525314,
    "source_hash": "c6342017",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "markdown_text = \"# Baseline Classifier Evaluation Results\\n\\n\"\n",
    "markdown_text += \"\"\"\n",
    "> The following table shows the results of the baseline classifiers on the real and synthetic datasets.\n",
    "> The results are based on 5-fold cross-validation with default parameters.\n",
    "> The results are displayed in descending order of F1-Score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# make a markdown table from the results\n",
    "for dataset_name, results in RESULTS.items():\n",
    "    markdown_text += f\"## {dataset_name.capitalize()} Dataset\\n\\n\"\n",
    "    \n",
    "    # Create table header\n",
    "    markdown_text += \"| Classifier | CV Accuracy | Test Accuracy | Test F1-Score | Training Time (s) | Efficiency (F1/s) |\\n\"\n",
    "    markdown_text += \"|------------|-------------|---------------|---------------|------------------| ------------------ |\\n\"\n",
    "    \n",
    "    # Order by f1-score\n",
    "    results_ord = {k: v for k, v in sorted(results.items(), key=lambda item: item[1]['test_f1'], reverse=True)}\n",
    "\n",
    "    # Add each classifier's results as a row\n",
    "    for clf_name, metrics in results_ord.items():\n",
    "        training_time_to_f1_ratio = metrics['test_f1'] / metrics['training_time']\n",
    "        markdown_text += f\"| {clf_name} | {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f}) | \"\n",
    "        markdown_text += f\"{metrics['test_accuracy']:.3f} | {metrics['test_f1']:.3f} | {metrics['training_time']:.2f} | {training_time_to_f1_ratio:.2f} |\\n\"\n",
    "    markdown_text += \"\\n\"  # Add space between dataset tables\n",
    "\n",
    "# Write to file\n",
    "with open('classification_results.md', 'w') as f:\n",
    "    f.write(markdown_text)\n",
    "\n",
    "print(\"Results have been written to classification_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import VotingClassifier, StackingClassifier\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# No need to reload RESULTS as it's already loaded in the notebook\n",
    "with open(\"results.pkl\", \"rb\") as f:\n",
    "    RESULTS = pickle.load(f)\n",
    "\n",
    "# Collect all models and their metrics across all datasets\n",
    "all_models = []\n",
    "for dataset_name, dataset_results in RESULTS.items():\n",
    "    for clf_name, metrics in dataset_results.items():\n",
    "        all_models.append({\n",
    "            'dataset': dataset_name,\n",
    "            'classifier': clf_name,\n",
    "            'model': metrics['fitted_model'],\n",
    "            'test_f1': metrics['test_f1'],\n",
    "            'test_accuracy': metrics['test_accuracy'],\n",
    "            'training_time': metrics['training_time']\n",
    "        })\n",
    "\n",
    "# Sort models by F1-score (descending)\n",
    "all_models_sorted = sorted(all_models, key=lambda x: x['test_f1'], reverse=True)\n",
    "\n",
    "# Select top 5 models \n",
    "top_models = all_models_sorted[:5]\n",
    "print(\"Top 5 models selected for ensemble:\")\n",
    "for i, model in enumerate(top_models):\n",
    "    print(f\"{i+1}. {model['dataset']} - {model['classifier']}: F1={model['test_f1']:.3f}\")\n",
    "\n",
    "# Create estimators for Voting Classifier\n",
    "estimators = [(f\"{model['dataset']}_{model['classifier']}\", model['model']) for model in top_models]\n",
    "\n",
    "# Create ensemble - try soft voting for classifiers that support predict_proba\n",
    "try:\n",
    "    # Check which models support predict_proba\n",
    "    support_proba = []\n",
    "    for name, model in estimators:\n",
    "        try:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                support_proba.append(True)\n",
    "            else:\n",
    "                support_proba.append(False)\n",
    "        except:\n",
    "            support_proba.append(False)\n",
    "    \n",
    "    # Use soft voting if all models support it, otherwise hard\n",
    "    voting_method = 'soft' if all(support_proba) else 'hard'\n",
    "    print(f\"Using {voting_method} voting method\")\n",
    "    \n",
    "    # Create weighted ensemble\n",
    "    weights = [model['test_f1'] for model in top_models]\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting=voting_method, weights=weights)\n",
    "except:\n",
    "    print(\"Falling back to hard voting without weights\")\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting='hard')\n",
    "\n",
    "# Prepare also a Stacking Classifier\n",
    "stacking = StackingClassifier(\n",
    "    estimators=estimators,\n",
    "    final_estimator=RESULTS['real+synthetic']['KNN']['fitted_model'],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Get test data from the real dataset\n",
    "X_test = DATASETS[\"real\"][\"X\"][\"test\"]\n",
    "y_test = DATASETS[\"real\"][\"y\"][\"test\"]\n",
    "X_train = DATASETS[\"real\"][\"X\"][\"train\"]\n",
    "y_train = DATASETS[\"real\"][\"y\"][\"train\"]\n",
    "\n",
    "# Fit and evaluate VotingClassifier\n",
    "start_time = time()\n",
    "ensemble.fit(X_train, y_train)\n",
    "ensemble_pred = ensemble.predict(X_test)\n",
    "ensemble_time = time() - start_time\n",
    "\n",
    "# Fit and evaluate StackingClassifier\n",
    "start_time = time()\n",
    "stacking.fit(X_train, y_train)\n",
    "stacking_pred = stacking.predict(X_test)\n",
    "stacking_time = time() - start_time\n",
    "\n",
    "# Calculate metrics\n",
    "ensemble_accuracy = accuracy_score(y_test, ensemble_pred)\n",
    "ensemble_precision, ensemble_recall, ensemble_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, ensemble_pred, average='macro'\n",
    ")\n",
    "\n",
    "stacking_accuracy = accuracy_score(y_test, stacking_pred)\n",
    "stacking_precision, stacking_recall, stacking_f1, _ = precision_recall_fscore_support(\n",
    "    y_test, stacking_pred, average='macro'\n",
    ")\n",
    "\n",
    "print(\"\\nVotingClassifier Performance:\")\n",
    "print(f\"  Accuracy: {ensemble_accuracy:.3f}\")\n",
    "print(f\"  F1-Score: {ensemble_f1:.3f}\")\n",
    "print(f\"  Precision: {ensemble_precision:.3f}\")\n",
    "print(f\"  Recall: {ensemble_recall:.3f}\")\n",
    "\n",
    "print(\"\\nStackingClassifier Performance:\")\n",
    "print(f\"  Accuracy: {stacking_accuracy:.3f}\")\n",
    "print(f\"  F1-Score: {stacking_f1:.3f}\")\n",
    "print(f\"  Precision: {stacking_precision:.3f}\")\n",
    "print(f\"  Recall: {stacking_recall:.3f}\")\n",
    "\n",
    "# Display detailed classification reports\n",
    "class_names = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I',\n",
    "               'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n",
    "\n",
    "print(\"\\nVotingClassifier Classification Report:\")\n",
    "print(classification_report(y_test, ensemble_pred, target_names=class_names))\n",
    "\n",
    "print(\"\\nStackingClassifier Classification Report:\")\n",
    "print(classification_report(y_test, stacking_pred, target_names=class_names))\n",
    "\n",
    "# Compare performance with individual models\n",
    "results_comparison = pd.DataFrame([\n",
    "    {'Model': 'VotingClassifier', 'Accuracy': ensemble_accuracy, 'F1-Score': ensemble_f1, \n",
    "     'Precision': ensemble_precision, 'Recall': ensemble_recall, 'Time': ensemble_time},\n",
    "    {'Model': 'StackingClassifier', 'Accuracy': stacking_accuracy, 'F1-Score': stacking_f1, \n",
    "     'Precision': stacking_precision, 'Recall': stacking_recall, 'Time': stacking_time}\n",
    "])\n",
    "\n",
    "# Use concat instead of append (which is deprecated)\n",
    "for model in top_models:\n",
    "    model_df = pd.DataFrame({\n",
    "        'Model': [f\"{model['dataset']}_{model['classifier']}\"],\n",
    "        'Accuracy': [model['test_accuracy']],\n",
    "        'F1-Score': [model['test_f1']],\n",
    "        'Precision': [None],  # We don't have this data readily available\n",
    "        'Recall': [None]      # We don't have this data readily available\n",
    "    })\n",
    "    results_comparison = pd.concat([results_comparison, model_df], ignore_index=True)\n",
    "\n",
    "print(\"\\nModel Comparison:\")\n",
    "display(results_comparison)\n",
    "\n",
    "# Save the best ensemble model\n",
    "best_ensemble = ensemble if ensemble_f1 > stacking_f1 else stacking\n",
    "best_name = \"voting_ensemble\" if ensemble_f1 > stacking_f1 else \"stacking_ensemble\"\n",
    "with open(f'{best_name}_classifier.pkl', 'wb') as f:\n",
    "    pickle.dump(best_ensemble, f)\n",
    "print(f\"\\nBest ensemble classifier ({best_name}) saved to '{best_name}_classifier.pkl'\")\n",
    "\n",
    "# Plot performance comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x='Model', y='F1-Score', data=results_comparison)\n",
    "plt.title('F1-Score Comparison Between Ensemble Models and Individual Models')\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "01226e7f933e4d559acd12b410b72184",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 2766,
    "execution_start": 1746785525554,
    "source_hash": "e07c0a6a",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get the XGBoost model and sample data\n",
    "xgb_model = RESULTS['real']['XGBoost']['fitted_model']\n",
    "\n",
    "# Create a SHAP explainer for the model\n",
    "explainer = shap.TreeExplainer(xgb_model)\n",
    "\n",
    "# Calculate SHAP values for a subset of training data (100 samples for visualization)\n",
    "shap_values = explainer.shap_values(X_train[:100])\n",
    "\n",
    "# Create summary plot\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(shap_values, X_train[:100], feature_names=X_train.columns, show=False)\n",
    "plt.title('SHAP Summary Plot for XGBoost Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Bar plot of mean absolute SHAP values\n",
    "plt.figure(figsize=(12, 6))\n",
    "shap.summary_plot(shap_values, X_train[:100], feature_names=X_train.columns, plot_type=\"bar\", show=False)\n",
    "plt.title('Mean Impact on Model Output (Mean Absolute SHAP Values)')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ab3f751cfc624b8c8514b078e0fe2202",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 286,
    "execution_start": 1746785528374,
    "source_hash": "857cff0c",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Create a dataframe from the results\n",
    "data = []\n",
    "for dataset_name, results in RESULTS.items():\n",
    "    for clf_name, metrics in results.items():\n",
    "        data.append({\n",
    "            'Dataset': dataset_name.capitalize(),\n",
    "            'Classifier': clf_name,\n",
    "            'CV Accuracy': metrics['cv_accuracy'],\n",
    "            'Test Accuracy': metrics['test_accuracy'],\n",
    "            'Test F1-Score': metrics['test_f1']\n",
    "        })\n",
    "\n",
    "df_metrics = pd.DataFrame(data)\n",
    "\n",
    "# Create a grouped bar plot\n",
    "plt.figure(figsize=(15, 8))\n",
    "metrics = ['CV Accuracy', 'Test Accuracy', 'Test F1-Score']\n",
    "x = np.arange(len(df_metrics['Classifier'].unique()))\n",
    "width = 0.25\n",
    "\n",
    "for i, dataset in enumerate(df_metrics['Dataset'].unique()):\n",
    "    dataset_data = df_metrics[df_metrics['Dataset'] == dataset]\n",
    "    plt.bar(x + i*width, dataset_data['Test F1-Score'], \n",
    "            width, label=f'{dataset}', alpha=0.8)\n",
    "\n",
    "plt.xlabel('Classifier')\n",
    "plt.ylabel('F1-Score')\n",
    "plt.title('Classifier Performance Comparison Across Datasets')\n",
    "plt.xticks(x + width, df_metrics['Classifier'].unique(), ha='center')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "f320505e83844c63a1de314774d561a3",
    "deepnote_cell_type": "code",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 318,
    "execution_start": 1746785528714,
    "source_hash": "74effa76"
   },
   "outputs": [],
   "source": [
    "# Evaluate the RandomForest model on real+synthetic dataset\n",
    "# Shows confusion matrix \n",
    "# Showsand per-class F1/precision/recall (imbalanced)\n",
    "\n",
    "def evaluate_model_with_report(model, X_test, y_test, class_names):\n",
    "    from sklearn.metrics import confusion_matrix, classification_report\n",
    "    import matplotlib.pyplot as plt\n",
    "    import seaborn as sns\n",
    "    import pandas as pd\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"Actual\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    report = classification_report(y_test, y_pred, target_names=class_names, output_dict=True)\n",
    "    return pd.DataFrame(report).transpose()\n",
    "\n",
    "model = RESULTS[\"real+synthetic\"][\"RandomForest\"][\"fitted_model\"]\n",
    "X_test = DATASETS[\"real+synthetic\"][\"X\"][\"test\"]\n",
    "y_test = DATASETS[\"real+synthetic\"][\"y\"][\"test\"]\n",
    "\n",
    "class_names = ['Insufficient_Weight', 'Normal_Weight', 'Overweight_Level_I',\n",
    "               'Overweight_Level_II', 'Obesity_Type_I', 'Obesity_Type_II', 'Obesity_Type_III']\n",
    "\n",
    "report_df = evaluate_model_with_report(model, X_test, y_test, class_names)\n",
    "display(report_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "75b138d25c0c410cbf4e475c6d32ff04",
    "deepnote_cell_type": "code",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 135,
    "execution_start": 1746785529105,
    "source_hash": "a61d1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the test labels\n",
    "y_test = DATASETS[\"real+synthetic\"][\"y\"][\"test\"]\n",
    "\n",
    "# LabelEncoder initialisieren und fitten\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "])\n",
    "\n",
    "# Zielklassen im Testset dekodieren\n",
    "y_test_named = pd.Series(label_encoder.inverse_transform(y_test), name=\"Klasse\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=y_test_named, order=y_test_named.value_counts().index)\n",
    "plt.title(\"Verteilung der Zielklassen im Testset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Zielklasse\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "created_in_deepnote_cell": true,
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a441f35e-4b4c-4c50-b56a-1aea6b800ed8' target=\"_blank\">\n",
    "<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\n",
    "Created in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>\n"
   ]
  }
 ],
 "metadata": {
  "deepnote_notebook_id": "9e537b3ff2d6483d8289e5b20d366294",
  "deepnote_persisted_session": {
   "createdAt": "2025-05-07T20:41:08.220Z"
  },
  "kernelspec": {
   "display_name": "datamining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
