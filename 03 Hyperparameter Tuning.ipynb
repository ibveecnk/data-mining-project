{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "c97d3",
    "cell_id": "29571d2f5f6d47098e0c69b07616734a",
    "deepnote_cell_type": "code",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 0,
    "execution_start": 1746785309355,
    "source_hash": "39219d5e"
   },
   "outputs": [],
   "source": [
    "# Models to try based on the results\n",
    "# 1. Random Forest Classifier\n",
    "# 2. LightGBM Classifier\n",
    "# 3. XGBoost Classifier\n",
    "# 4. LinearDiscriminantAnalysis\n",
    "# 5. KNN?\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from time import time\n",
    "from sklearn.model_selection import cross_validate, LeaveOneOut\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, make_scorer, precision_score, recall_score, f1_score\n",
    "\n",
    "# Models\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score, make_scorer, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "5c865"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils.validation import check_is_fitted, validate_data\n",
    "\n",
    "class BaselineModel(BaseEstimator):\n",
    "    def fit(self, X, y):\n",
    "        # Ensure y is 1D\n",
    "        y = np.ravel(y)\n",
    "        self._majority_class = pd.Series(y).mode()[0]\n",
    "        self._is_fitted = True\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        check_is_fitted(self, '_is_fitted')\n",
    "        return np.full(shape=(X.shape[0],), fill_value=self._majority_class)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "eb0b8",
    "cell_id": "cfdef7374b6a48998003d74987ebf89d",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 213888,
    "execution_start": 1746785311374,
    "source_hash": "808af377",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_evaluate_classifiers(X_train, y_train, X_test, y_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Train and evaluate multiple classifiers using 5-fold cross validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : pd.DataFrame\n",
    "        Training features\n",
    "    y_train : pd.Series\n",
    "        Training labels\n",
    "    X_test : pd.DataFrame\n",
    "        Test features\n",
    "    y_test : pd.Series\n",
    "        Test labels\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing results for each classifier\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    # Define classifiers\n",
    "    old_classifiers = {\n",
    "        \"RandomForest\": {\n",
    "            \"model\": RandomForestClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'max_depth': np.arange(3, 11, dtype=int),\n",
    "                'min_samples_split': np.linspace(0.1, 1.0, 10),\n",
    "                'min_samples_leaf': np.linspace(0.1, 0.5, 5),\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "                'bootstrap': [True, False],\n",
    "            }\n",
    "        },\n",
    "        \"GradientBoosting\": {\n",
    "            \"model\": GradientBoostingClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'learning_rate': np.linspace(0.01, 0.3, 10),\n",
    "                'max_depth': np.arange(3, 11, dtype=int),\n",
    "                'min_samples_split': np.linspace(0.1, 1.0, 10),\n",
    "                'min_samples_leaf': np.linspace(0.1, 0.5, 5),\n",
    "                'max_features': ['sqrt', 'log2'],\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBClassifier(\n",
    "                random_state=random_state, \n",
    "                eval_metric='mlogloss',\n",
    "\n",
    "            ),\n",
    "            \"params\": {\n",
    "                'max_depth': np.linspace(3, 11, dtype=int),\n",
    "                'learning_rate': np.linspace(0.1, 0.3, 10),\n",
    "                'n_estimators': np.arange(100, 1000, 100),\n",
    "                'gamma': np.linspace(0, 5, 6),\n",
    "            }\n",
    "        },\n",
    "        \"$k$NN\": {\n",
    "            \"model\": KNeighborsClassifier(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': np.arange(1, 31),\n",
    "                'weights': ['uniform', 'distance'],\n",
    "                'algorithm': ['auto', 'ball_tree', 'kd_tree', 'brute'],\n",
    "                'leaf_size': np.arange(10, 100, 10),\n",
    "                \"p\": [1, 2]\n",
    "            }\n",
    "        },\n",
    "        \"LGBM\": {\n",
    "            \"model\": LGBMClassifier(random_state=random_state, verbosity=-1, verbose=-1),\n",
    "            \"params\": {\n",
    "                'num_leaves': np.arange(20, 130, 10),\n",
    "                'reg_alpha': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                'lambda_l1': [0, 1, 1.5],\n",
    "                'lambda_l2': [0, 1],\n",
    "                }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'max_depth': np.arange(4, 11, dtype=int),\n",
    "                'min_samples_split': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                'min_samples_leaf': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "                'criterion': ['gini', 'log_loss'],\n",
    "            }\n",
    "        },\n",
    "        \"Baseline\": {\n",
    "            \"model\": BaselineModel(),\n",
    "            \"params\": {}\n",
    "        },\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    # Define classifiers with reduced hyperparameters\n",
    "    classifiers = {\n",
    "        \"RandomForest\": {\n",
    "            \"model\": RandomForestClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 500],\n",
    "                'max_depth': [None, 3, 5, 7],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'min_samples_leaf': [1, 2],\n",
    "            }\n",
    "        },\n",
    "        \"GradientBoosting\": {\n",
    "            \"model\": GradientBoostingClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'n_estimators': [100, 200, 500],\n",
    "                'learning_rate': [0.01, 0.1, 0.2],\n",
    "                'max_depth': [3, 5, 7],\n",
    "            }\n",
    "        },\n",
    "        \"XGBoost\": {\n",
    "            \"model\": XGBClassifier(\n",
    "                random_state=random_state, \n",
    "                eval_metric='mlogloss',\n",
    "            ),\n",
    "            \"params\": {\n",
    "                'max_depth': [3, 5, 7],\n",
    "                'learning_rate': [0.1, 0.2],\n",
    "                'n_estimators': [100, 200, 500],\n",
    "            }\n",
    "        },\n",
    "        \"$k$NN\": {\n",
    "            \"model\": KNeighborsClassifier(),\n",
    "            \"params\": {\n",
    "                'n_neighbors': [3, 5, 10, 20],\n",
    "                'weights': ['uniform', 'distance'],\n",
    "            }\n",
    "        },\n",
    "        \"LGBM\": {\n",
    "            \"model\": LGBMClassifier(random_state=random_state, verbose=-1),\n",
    "            \"params\": {\n",
    "                'num_leaves': [30, 50, 100, 150],\n",
    "                'reg_alpha': [0.1, 0.5],\n",
    "                'lambda_l1': [0, 1],\n",
    "            }\n",
    "        },\n",
    "        \"Decision Tree\": {\n",
    "            \"model\": DecisionTreeClassifier(random_state=random_state),\n",
    "            \"params\": {\n",
    "                'max_depth': [None, 5, 10],\n",
    "                'min_samples_split': [2, 5],\n",
    "                'criterion': ['gini', 'entropy'],\n",
    "            }\n",
    "        },\n",
    "        \"MLPClassifier\": {\n",
    "            \"model\": MLPClassifier(random_state=random_state, max_iter=1000),\n",
    "            \"params\": {\n",
    "                'hidden_layer_sizes': [(50,), (100,), (50, 50), (100, 50)],\n",
    "                'activation': ['relu', 'tanh', 'logistic'],\n",
    "                'alpha': [0.0001, 0.001, 0.01],\n",
    "                'learning_rate': ['constant', 'adaptive'],\n",
    "            }\n",
    "        },\n",
    "        \"Baseline\": {\n",
    "            \"model\": BaselineModel(),\n",
    "            \"params\": {}\n",
    "        },\n",
    "    }\n",
    "    \n",
    "    # Define custom scoring functions with zero_division=0\n",
    "    def precision_scorer(y_true, y_pred):\n",
    "        return precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    def recall_scorer(y_true, y_pred):\n",
    "        return recall_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    def f1_scorer(y_true, y_pred):\n",
    "        return f1_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    TRIALS = 10\n",
    "\n",
    "    # Create scorers\n",
    "    scoring = {\n",
    "        'accuracy': make_scorer(accuracy_score),\n",
    "        'precision_macro': make_scorer(precision_score, average='macro'),\n",
    "        'recall_macro': make_scorer(recall_score, average='macro'),\n",
    "        'f1_macro': make_scorer(f1_score, average='macro')\n",
    "    }\n",
    "\n",
    "    results = {}\n",
    "    \n",
    "    for name, clf_dict in classifiers.items():\n",
    "        print(f\"Training {name} classifier...\")\n",
    "        # Start timing\n",
    "        start_time = time()\n",
    "        \n",
    "        # Get the model from the dictionary\n",
    "        clf = clf_dict['model']\n",
    "        \n",
    "        # For small datasets or datasets with tiny classes\n",
    "        if X_train.shape[0] < 500:\n",
    "            cv = LeaveOneOut()  # Use LOO for very small datasets\n",
    "        else:\n",
    "            # Count samples in smallest class\n",
    "            class_counts = y_train.value_counts()\n",
    "            min_class_samples = class_counts.min()\n",
    "            \n",
    "\n",
    "            n_splits = min(5, min_class_samples)  # Use at most 5 splits, but no more than samples in smallest class\n",
    "            print(f\"Using {n_splits} splits due to small class size ({min_class_samples} samples in smallest class)\")\n",
    "            cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "\n",
    "            # # Choose appropriate number of splits based on smallest class\n",
    "            # if min_class_samples < 10:\n",
    "            #     n_splits = min(5, min_class_samples)  # Use at most 5 splits, but no more than samples in smallest class\n",
    "            #     print(f\"Using {n_splits} splits due to small class size ({min_class_samples} samples in smallest class)\")\n",
    "            #     cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=random_state)\n",
    "            # else:\n",
    "            #     cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=random_state)\n",
    "        \n",
    "        # Perform cross-validation with random search\n",
    "        cv_results = RandomizedSearchCV(clf, \n",
    "                                        clf_dict['params'], \n",
    "                                        cv=cv, \n",
    "                                        scoring=scoring, \n",
    "                                        refit='f1_macro', \n",
    "                                        n_iter=TRIALS,\n",
    "                                        n_jobs=-1, # todo\n",
    "                                        random_state=random_state, \n",
    "                                        verbose=4)\n",
    "        cv_results.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # Fit on full training data and evaluate on test set\n",
    "        best_clf = cv_results.best_estimator_\n",
    "        test_pred_encoded = best_clf.predict(X_test)\n",
    "\n",
    "        test_scores = precision_recall_fscore_support(y_test.values, test_pred_encoded, average='macro', zero_division=0)\n",
    "        test_accuracy = accuracy_score(y_test.values.ravel(), test_pred_encoded)\n",
    "        \n",
    "        # Store results\n",
    "        results[name] = {\n",
    "            'cv_accuracy': cv_results.cv_results_['mean_test_accuracy'].mean(),\n",
    "            'cv_accuracy_std': cv_results.cv_results_['std_test_accuracy'].mean(),\n",
    "            'cv_precision': cv_results.cv_results_['mean_test_precision_macro'].mean(),\n",
    "            'cv_recall': cv_results.cv_results_['mean_test_recall_macro'].mean(),\n",
    "            'cv_f1': cv_results.cv_results_['mean_test_f1_macro'].mean(),\n",
    "            'test_accuracy': test_accuracy,\n",
    "            'test_precision': test_scores[0],\n",
    "            'test_recall': test_scores[1],\n",
    "            'test_f1': test_scores[2],\n",
    "            'training_time': time() - start_time,\n",
    "            'fitted_model': best_clf,\n",
    "            'best_params': cv_results.best_params_\n",
    "        }\n",
    "        \n",
    "        print(f\"DONE: {results[name]}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "RESULTS = {}\n",
    "\n",
    "# Train and test the model for all datasets\n",
    "approaches = {'Initial': ['real_data', 'synth_data'], 'Extra': ['real_data', 'real_pseudoreal_data', 'real_pseudoreal_synth_data']}\n",
    "\n",
    "for approach, datasets in approaches.items():\n",
    "    for dataset in datasets:\n",
    "        # Load the preprocessed data\n",
    "        X_train = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/X_train.csv\")\n",
    "        y_train = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/y_train.csv\")\n",
    "        X_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/X_test.csv\")\n",
    "        y_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset}/y_test.csv\")\n",
    "        \n",
    "\n",
    "        print(f\"\\nEvaluating on {approach}_{dataset} dataset:\")\n",
    "\n",
    "        # Train and evaluate\n",
    "        results = train_evaluate_classifiers(X_train, y_train, X_test, y_test)\n",
    "        \n",
    "        # Store results\n",
    "        RESULTS[f\"{approach}_{dataset}\"] = results\n",
    "\n",
    "        # Print results\n",
    "        for clf_name, metrics in results.items():\n",
    "            print(f\"\\n{clf_name}:\")\n",
    "            print(f\"CV Accuracy: {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f})\")\n",
    "            print(f\"Test Accuracy: {metrics['test_accuracy']:.3f}\")\n",
    "            print(f\"Test F1-Score: {metrics['test_f1']:.3f}\")\n",
    "            print(f\"Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "\n",
    "        results_df = pd.DataFrame.from_dict(results, orient='index')\n",
    "        print(results_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "f32c6"
   },
   "outputs": [],
   "source": [
    "# Save results to global variable\n",
    "import pickle\n",
    "with open(\"tuned-results.pkl\", \"wb\") as f:\n",
    "    pickle.dump(RESULTS, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "13ab6",
    "cell_id": "9335436e7ba5443faff31489f2c6a56c",
    "deepnote_cell_type": "visualization",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 1,
    "execution_start": 1746786603404,
    "source_hash": "b623e53d"
   },
   "outputs": [],
   "source": [
    "for dataset_name, dataset_results in RESULTS.items():\n",
    "    print(f\"Results for {dataset_name.capitalize()} Dataset:\\n\")\n",
    "    for clf_name, metrics in dataset_results.items():\n",
    "        print(f\"Classifier: {clf_name}\")\n",
    "        print(f\"  CV Accuracy: {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f})\")\n",
    "        print(f\"  CV Precision: {metrics['cv_precision']:.3f}\")\n",
    "        print(f\"  CV Recall: {metrics['cv_recall']:.3f}\")\n",
    "        print(f\"  CV F1-Score: {metrics['cv_f1']:.3f}\")\n",
    "        print(f\"  Test Accuracy: {metrics['test_accuracy']:.3f}\")\n",
    "        print(f\"  Test F1-Score: {metrics['test_f1']:.3f}\")\n",
    "        print(f\"  Training Time: {metrics['training_time']:.2f} seconds\")\n",
    "        print(f\"  Best Parameters: {metrics['best_params']}\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "42312",
    "cell_id": "68543a86dde043478b6cfc9492519366",
    "deepnote_cell_type": "code",
    "deepnote_variable_name": "",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 188,
    "execution_start": 1746785525314,
    "source_hash": "c6342017",
    "sql_integration_id": ""
   },
   "outputs": [],
   "source": [
    "markdown_text = \"# Hyperparameter-Tuned Classifier Evaluation Results\\n\\n\"\n",
    "markdown_text += \"\"\"\n",
    "> The following table shows the results of the baseline classifiers on the real and synthetic datasets.\n",
    "> The results are based on 5-fold cross-validation with default parameters.\n",
    "> The results are displayed in descending order of F1-Score.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# make a markdown table from the results\n",
    "for dataset_name, results in RESULTS.items():\n",
    "    markdown_text += f\"## {dataset_name.capitalize()} Dataset\\n\\n\"\n",
    "    \n",
    "    # Create table header\n",
    "    markdown_text += \"| Classifier | CV Accuracy | Test Accuracy | Test F1-Score | Training Time (s) | Efficiency (F1/s) |\\n\"\n",
    "    markdown_text += \"|------------|-------------|---------------|---------------|------------------| ------------------ |\\n\"\n",
    "    \n",
    "    # Order by f1-score\n",
    "    results_ord = {k: v for k, v in sorted(results.items(), key=lambda item: item[1]['test_f1'], reverse=True)}\n",
    "\n",
    "    # Add each classifier's results as a row\n",
    "    for clf_name, metrics in results_ord.items():\n",
    "        training_time_to_f1_ratio = metrics['test_f1'] / metrics['training_time']\n",
    "        markdown_text += f\"| {clf_name} | {metrics['cv_accuracy']:.3f} (±{metrics['cv_accuracy_std']:.3f}) | \"\n",
    "        markdown_text += f\"{metrics['test_accuracy']:.3f} | {metrics['test_f1']:.3f} | {metrics['training_time']:.2f} | {training_time_to_f1_ratio:.2f} |\\n\"\n",
    "    markdown_text += \"\\n\"  # Add space between dataset tables\n",
    "\n",
    "# Write to file\n",
    "with open('tuning_results.md', 'w') as f:\n",
    "    f.write(markdown_text)\n",
    "\n",
    "print(\"Results have been written to tuning_results.md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellUniqueIdByVincent": "b98fa",
    "cell_id": "75b138d25c0c410cbf4e475c6d32ff04",
    "deepnote_cell_type": "code",
    "execution_context_id": "e9678d9d-562a-4883-be5d-d520614573e0",
    "execution_millis": 135,
    "execution_start": 1746785529105,
    "source_hash": "a61d1e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load the test labels\n",
    "y_test = DATASETS[\"real+synthetic\"][\"y\"][\"test\"]\n",
    "\n",
    "# LabelEncoder initialisieren und fitten\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit([\n",
    "    'Insufficient_Weight',\n",
    "    'Normal_Weight',\n",
    "    'Overweight_Level_I',\n",
    "    'Overweight_Level_II',\n",
    "    'Obesity_Type_I',\n",
    "    'Obesity_Type_II',\n",
    "    'Obesity_Type_III'\n",
    "])\n",
    "\n",
    "# Zielklassen im Testset dekodieren\n",
    "y_test_named = pd.Series(label_encoder.inverse_transform(y_test), name=\"Klasse\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=y_test_named, order=y_test_named.value_counts().index)\n",
    "plt.title(\"Verteilung der Zielklassen im Testset\")\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel(\"Zielklasse\")\n",
    "plt.ylabel(\"Anzahl\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vincent": {
   "sessionId": "f2f1d6aa85835c35bda394be_2025-05-15T12-42-33-070Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
