{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d1a0e90",
   "metadata": {
    "cellUniqueIdByVincent": "53db8"
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83cf6b1b",
   "metadata": {
    "cellUniqueIdByVincent": "10df8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.metrics import confusion_matrix, multilabel_confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311f6870",
   "metadata": {
    "cellUniqueIdByVincent": "0695d"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eceb74b2",
   "metadata": {
    "cellUniqueIdByVincent": "49153"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can't get attribute 'BaselineModel' on <module '__main__'>",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Load pickle file with results\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mtuned-results.pkl\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mrb\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     results = \u001b[43mpickle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m plots = {\n\u001b[32m      7\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mInitial\u001b[39m\u001b[33m'\u001b[39m: {},\n\u001b[32m      8\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mExtra\u001b[39m\u001b[33m'\u001b[39m: {}\n\u001b[32m      9\u001b[39m }\n\u001b[32m     11\u001b[39m target_map = {\n\u001b[32m     12\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mInsufficient_Weight\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m0\u001b[39m,\n\u001b[32m     13\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mNormal_Weight\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m1\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     18\u001b[39m                 \u001b[33m'\u001b[39m\u001b[33mObesity_Type_III\u001b[39m\u001b[33m'\u001b[39m: \u001b[32m6\u001b[39m\n\u001b[32m     19\u001b[39m             }\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get attribute 'BaselineModel' on <module '__main__'>"
     ]
    }
   ],
   "source": [
    "# Load pickle file with results\n",
    "with open('tuned-results.pkl', 'rb') as f:\n",
    "    results = pickle.load(f)\n",
    "\n",
    "\n",
    "plots = {\n",
    "    'Initial': {},\n",
    "    'Extra': {}\n",
    "}\n",
    "\n",
    "target_map = {\n",
    "                'Insufficient_Weight': 0,\n",
    "                'Normal_Weight': 1,\n",
    "                'Overweight_Level_I': 2,\n",
    "                'Overweight_Level_II': 3,\n",
    "                'Obesity_Type_I': 4,\n",
    "                'Obesity_Type_II': 5,\n",
    "                'Obesity_Type_III': 6\n",
    "            }\n",
    "        \n",
    "reversed_target_map = {v: k for k, v in target_map.items()}\n",
    "\n",
    "# Print the results\n",
    "for dataset, models in results.items():\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    approach = dataset.split('_')[0]\n",
    "    dataset_name = \"_\".join(dataset.split('_')[1:])\n",
    "    X_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset_name}/X_test.csv\")\n",
    "    y_test = pd.read_csv(f\"./datasets/preprocessed/{approach}/{dataset_name}/y_test.csv\")\n",
    "\n",
    "    if approach not in plots:\n",
    "        plots[approach] = {}\n",
    "\n",
    "    for model_name, model_results in models.items():\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        y_pred = model_results['fitted_model'].predict(X_test)\n",
    "        \n",
    "        if model_name not in plots[approach]:\n",
    "            plots[approach][model_name] = {}\n",
    "\n",
    "        if dataset_name not in plots[approach][model_name]:\n",
    "            plots[approach][model_name][dataset_name] = {}\n",
    "\n",
    "        \n",
    "\n",
    "        plots[approach][model_name][dataset_name]['f1'] = model_results['test_f1']\n",
    "\n",
    "        # Convert predictions from integers to string labels\n",
    "        y_pred_labels = np.array([reversed_target_map[label] for label in y_pred])\n",
    "        \n",
    "        # Convert y_test to string labels too\n",
    "        y_test_labels = np.array([reversed_target_map[int(label)] for label in y_test.values.flatten()])\n",
    "        \n",
    "        # Create a DataFrame for the confusion matrix with proper labels\n",
    "        cm_array = confusion_matrix(y_test_labels, y_pred_labels, labels=list(reversed_target_map.values()))\n",
    "        \n",
    "        \n",
    "        # Store the confusion matrix in the dictionary\n",
    "        plots[approach][model_name][dataset_name]['cm'] = cm_array\n",
    "        \n",
    "\n",
    "        # Feature importance\n",
    "        if hasattr(model_results['fitted_model'], 'feature_importances_'):\n",
    "                plots[approach][model_name][dataset_name]['fi'] = pd.DataFrame({\n",
    "                    'Feature': X_test.columns,\n",
    "                    'Importance': model_results['fitted_model'].feature_importances_\n",
    "                }).sort_values(by='Importance', ascending=False)\n",
    "        elif hasattr(model_results['fitted_model'], 'coef_'):\n",
    "            plots[approach][model_name][dataset_name]['fi'] = pd.DataFrame({\n",
    "                'Feature': X_test.columns,\n",
    "                'Importance': np.abs(model_results['fitted_model'].coef_[0])\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "        else:\n",
    "            plots[approach][model_name][dataset_name]['fi'] = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5681cae6",
   "metadata": {
    "cellUniqueIdByVincent": "14f8d"
   },
   "outputs": [],
   "source": [
    "\n",
    "for approach, models in plots.items():\n",
    "    \n",
    "    for model_name, datasets in models.items():\n",
    "        i = 0\n",
    "        # Create a figure for the confusion matrix and feature importance\n",
    "        if approach == 'Initial':\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            fig, axes = plt.subplots(1, 3, figsize=(26, 8))\n",
    "            axes = axes.flatten()\n",
    "        \n",
    "        for dataset_name, model_results in datasets.items():\n",
    "            ax = axes[i]\n",
    "            print(f'{approach} - {model_name} - {dataset_name}')\n",
    "            # Plotting\n",
    "            sns.heatmap(model_results['cm'], annot=True, fmt='d', cmap='Blues', xticklabels=list(reversed_target_map.values()), yticklabels=list(reversed_target_map.values()), ax=ax)\n",
    "            ax.set_title(f'Confusion Matrix for {approach} - {dataset_name} - {model_name}, F1 Score: {model_results[\"f1\"]:.2f}')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "\n",
    "            i += 1\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143918d7",
   "metadata": {
    "cellUniqueIdByVincent": "76bcd"
   },
   "outputs": [],
   "source": [
    "# Plot feature importance\n",
    "for approach, models in plots.items():\n",
    "    for model_name, datasets in models.items():\n",
    "        for dataset_name, model_results in datasets.items():\n",
    "            if model_results['fi'] is not None:\n",
    "                plt.figure(figsize=(10, 6))\n",
    "                sns.barplot(x='Importance', y='Feature', data=model_results['fi'].head(10))\n",
    "                plt.title(f'Feature Importance for {approach} - {dataset_name} - {model_name}')\n",
    "                plt.xlabel('Importance')\n",
    "                plt.ylabel('Feature')\n",
    "                plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "806bd4bf",
   "metadata": {
    "cellUniqueIdByVincent": "0cec9"
   },
   "outputs": [],
   "source": [
    "# Classification report\n",
    "for dataset, models in results.items():\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    for model_name, model_results in models.items():\n",
    "        y_pred = model_results['fitted_model'].predict(X_test)\n",
    "        print(f\"  Model: {model_name}\")\n",
    "        print(classification_report(y_test, y_pred, zero_division=0, labels=list(target_map.values()), target_names=list(target_map.keys())))\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65588324",
   "metadata": {
    "cellUniqueIdByVincent": "68d36"
   },
   "outputs": [],
   "source": [
    "\n",
    "# 1) Flatten into a DataFrame\n",
    "records = []\n",
    "for dataset, models in results.items():\n",
    "    for model, metrics in models.items():\n",
    "        approach = dataset.split('_')[0]\n",
    "        dataset_name = \"_\".join(dataset.split('_')[1:])\n",
    "        records.append({\n",
    "            'approach': approach,\n",
    "            'dataset': dataset_name,\n",
    "            'model': model,\n",
    "            'f1': metrics['test_f1']\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame.from_records(records)\n",
    "\n",
    "# 2) Define the explicit color map for known datasets\n",
    "color_map = {\n",
    "    'real_data': 'green',\n",
    "    'real_pseudoreal_data': 'red',\n",
    "    # add other explicit mappings here…\n",
    "}\n",
    "\n",
    "# 3) Plotting helper\n",
    "def plot_approach(df, approach_name):\n",
    "    sub = df[df['approach'] == approach_name]\n",
    "    pivot = sub.pivot(index='model', columns='dataset', values='f1')\n",
    "    \n",
    "    # Build a colors list, falling back to the default cycle\n",
    "    default_cycle = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "    default_iter = iter(default_cycle)\n",
    "    colors = [\n",
    "        color_map.get(ds, next(default_iter))\n",
    "        for ds in pivot.columns\n",
    "    ]\n",
    "    \n",
    "    ax = pivot.plot(\n",
    "        kind='bar',\n",
    "        figsize=(8, 6),\n",
    "        width=0.8,\n",
    "        color=colors\n",
    "    )\n",
    "    ax.set_title(f'F1 Scores — {approach_name}')\n",
    "    ax.set_ylabel('Test F1 Score')\n",
    "    ax.set_xlabel('Model')\n",
    "    ax.legend(title='Dataset')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 4) Draw both charts\n",
    "plot_approach(df, 'Initial')\n",
    "plot_approach(df, 'Extra')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "vincent": {
   "sessionId": "8eeafe8d84bf8dd164155bd8_2025-05-15T12-42-20-539Z"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
